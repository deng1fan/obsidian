---
Type: article
Authors: Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, Siva Reddy
Year: 2022
Title: FaithDial: A Faithful Benchmark for Information-Seeking Dialogue
Journal: 
DOI: 10.48550/arXiv.2204.10757
Publisher: 
---

#  (2022)FaithDial: A Faithful Benchmark for Information-Seeking Dialogue
###                  ——Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, Siva Reddy
[*Read it now! See in Zotero*](zotero://select/items/@FaithDialFaithfulBenchmark2022dziri)
**Web:** [Open online](https://arxiv.org/abs/2204.10757v1)
**Citekey:** FaithDialFaithfulBenchmark2022dziri
**Tags:** #知识忠诚度 #粗读 #未完
**Code:** [*Available Here*](https://github.com/McGill-NLP/FaithDial)


## 摘要
The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.

## 总结
提出**FaithDial**数据集，与[[OriginHallucinationsConversational2022dziri]]属于孪生论文
![[Pasted image 20220706161617.png]]
```ad-attention
title: 注意
本文还提出一个子数据集：**FAITHCRITIC**，二分类数据集，分别来源于原数据集中标签为entailment（20k）和hallucination（14k）的数据，总计34k个utterances

```
```ad-attention
title: 注意
此数据集不涉及知识选择

```

数据格式：![[Pasted image 20220706202319.png]]

[项目地址](https://mcgill-nlp.github.io/FaithDial/)

## 研究动机


## 研究背景/ 问题描述


## 方法的创新点
本文给出的任务分两步：

第一个：
> providing high-quality data to generate faithful responses in information-seeking dialogue.     ([Dziri 等。, 2022, p. 6](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=6&annotation=2T4DKI49))  
> 笔记：回复生成

第二个：
> use the collected labels as training data for a critic to determine whether a given response is faithful or hallucinated.     ([Dziri 等。, 2022, p. 6](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=6&annotation=TFIZCNZ3))  
> 笔记：对回复进行幻觉判断：是否忠诚于给定知识（二分类任务）

```ad-attention
title: 注意

在回复生成任务中，使用**InfoNCE**辅助损失函数，为每个样本构建8个负样本（涉及动词替换和实体替换，并且人工标注的负样本也包括在内）
> To generate up to k = 8 negative candidates x<sup>−</sup>, we follow a perturb-and-generate strategy for each utterance in the training data.     ([Dziri 等。, 2022, p. 7](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=7&annotation=W8MHH82Z))  
> 笔记：使用扰动生成的策略进行负样本生成
```


## Baselines
[]()       [*Code*]()
> 

## 实验评测
![[Pasted image 20220706165712.png]]

![[Pasted image 20220706181804.png]]
```ad-note
title: 看法
CTRL的控制在大多数评价指标上都取得了最优效果，但是对流畅性相关的指标并不友好，例如BLEU和ROUGE，这个从侧面说明模型没有学会合理利用知识，或许可以作为以后优化的一个点

```
![[Pasted image 20220706182727.png]]
```ad-note
title: 看法
对比学习好强大👍🏻

```
![[Pasted image 20220706183541.png]]

![[Pasted image 20220706183656.png]]

## 实验结论


## 笔记
> two versions of the same dialogue turn, either hallucinated or faithful, can provide signal for (contrastive) learning and evidence for a linguistic analysis.     ([Dziri 等。, 2022, p. 3](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=3&annotation=DZG3BP7V))  
> 笔记：同样地上下文，不同类型的回复👍🏻

> WOW is relatively less hallucinated compared to CMU-DoG and TopicalChat. Moreover, full hallucinations—responses that contain no faithful content and that therefore need to be entirely thrown out— are highly prevalent in the latter two (61.4% in CMU-DoG and 46.8% in TopicalChat and only 19.7% in WOW). Moreover, knowledge snippets in WOW tend to be shorter, which is preferable as their length is correlated with increased hallucination due to the constrained cognitive capacity for text navigation and comprehension in humans     ([Dziri 等。, 2022, p. 3](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=3&annotation=D4P7R7LA))  
> 笔记：为什么选择WOW进行改写标注的原因：完全幻觉回复较少、知识短

> To ensure that the responses are creative, we disallowed workers from copying segments from K.     ([Dziri 等。, 2022, p. 4](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=4&annotation=SFEHZKB7))  
> 笔记：不允许直接拷贝知识

> Although the SEEKER has no restrictions on their utterances, it is inevitable that the conversation may drift away— because of the edits on the WIZARD’s response—making the existing SEEKER’s next utterance in WOW incoherent with the new context.     ([Dziri 等。, 2022, p. 4](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=4&annotation=3UDV5AMC))  
> 笔记：为了让后文与修改后的上文一致，需要修改Seeker的话语
> ![[Pasted image 20220706161150.png]]
> ![[Pasted image 20220706161203.png]]

> Finally, we adopt a training strategy, called loss truncation (Kang and Hashimoto, 2020) to cope with the presence of hallucination in WOW, by adaptively eliminating examples with a high training loss     ([Dziri 等。, 2022, p. 7](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=7&annotation=F84RRJ44))  
> 笔记：使用损失截断去除高损失样例的影响