---
Type: article
Authors: Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, Siva Reddy
Year: 2022
Title: FaithDial: A Faithful Benchmark for Information-Seeking Dialogue
Journal: 
DOI: 10.48550/arXiv.2204.10757
Publisher: 
---

#  (2022)FaithDial: A Faithful Benchmark for Information-Seeking Dialogue
###                  â€”â€”Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M. Ponti, Siva Reddy
[*Read it now! See in Zotero*](zotero://select/items/@FaithDialFaithfulBenchmark2022dziri)
**Web:** [Open online](https://arxiv.org/abs/2204.10757v1)
**Citekey:** FaithDialFaithfulBenchmark2022dziri
**Tags:** #çŸ¥è¯†å¿ è¯šåº¦ #ç²—è¯» #æœªå®Œ
**Code:** [*Available Here*](https://github.com/McGill-NLP/FaithDial)


## æ‘˜è¦
The goal of information-seeking dialogue is to respond to seeker queries with natural language utterances that are grounded on knowledge sources. However, dialogue systems often produce unsupported utterances, a phenomenon known as hallucination. Dziri et al. (2022)'s investigation of hallucinations has revealed that existing knowledge-grounded benchmarks are contaminated with hallucinated responses at an alarming level (>60% of the responses) and models trained on this data amplify hallucinations even further (>80% of the responses). To mitigate this behavior, we adopt a data-centric solution and create FaithDial, a new benchmark for hallucination-free dialogues, by editing hallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe that FaithDial is more faithful than WoW while also maintaining engaging conversations. We show that FaithDial can serve as a training signal for: i) a hallucination critic, which discriminates whether an utterance is faithful or not, and boosts the performance by 21.1 F1 score on the BEGIN benchmark compared to existing datasets for dialogue coherence; ii) high-quality dialogue generation. We benchmark a series of state-of-the-art models and propose an auxiliary contrastive objective that achieves the highest level of faithfulness and abstractiveness based on several automated metrics. Further, we find that the benefits of FaithDial generalize to zero-shot transfer on other datasets, such as CMU-Dog and TopicalChat. Finally, human evaluation reveals that responses generated by models trained on FaithDial are perceived as more interpretable, cooperative, and engaging.

## æ€»ç»“
æå‡º**FaithDial**æ•°æ®é›†ï¼Œä¸[[OriginHallucinationsConversational2022dziri]]å±äºå­ªç”Ÿè®ºæ–‡
![[Pasted image 20220706161617.png]]
```ad-attention
title: æ³¨æ„
æœ¬æ–‡è¿˜æå‡ºä¸€ä¸ªå­æ•°æ®é›†ï¼š**FAITHCRITIC**ï¼ŒäºŒåˆ†ç±»æ•°æ®é›†ï¼Œåˆ†åˆ«æ¥æºäºåŸæ•°æ®é›†ä¸­æ ‡ç­¾ä¸ºentailmentï¼ˆ20kï¼‰å’Œhallucinationï¼ˆ14kï¼‰çš„æ•°æ®ï¼Œæ€»è®¡34kä¸ªutterances

```
```ad-attention
title: æ³¨æ„
æ­¤æ•°æ®é›†ä¸æ¶‰åŠçŸ¥è¯†é€‰æ‹©

```

æ•°æ®æ ¼å¼ï¼š![[Pasted image 20220706202319.png]]

[é¡¹ç›®åœ°å€](https://mcgill-nlp.github.io/FaithDial/)

## ç ”ç©¶åŠ¨æœº


## ç ”ç©¶èƒŒæ™¯/ é—®é¢˜æè¿°


## æ–¹æ³•çš„åˆ›æ–°ç‚¹
æœ¬æ–‡ç»™å‡ºçš„ä»»åŠ¡åˆ†ä¸¤æ­¥ï¼š

ç¬¬ä¸€ä¸ªï¼š
> providing high-quality data to generate faithful responses in information-seeking dialogue.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 6](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=6&annotation=2T4DKI49))  
> ç¬”è®°ï¼šå›å¤ç”Ÿæˆ

ç¬¬äºŒä¸ªï¼š
> use the collected labels as training data for a critic to determine whether a given response is faithful or hallucinated.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 6](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=6&annotation=TFIZCNZ3))  
> ç¬”è®°ï¼šå¯¹å›å¤è¿›è¡Œå¹»è§‰åˆ¤æ–­ï¼šæ˜¯å¦å¿ è¯šäºç»™å®šçŸ¥è¯†ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰

```ad-attention
title: æ³¨æ„

åœ¨å›å¤ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä½¿ç”¨**InfoNCE**è¾…åŠ©æŸå¤±å‡½æ•°ï¼Œä¸ºæ¯ä¸ªæ ·æœ¬æ„å»º8ä¸ªè´Ÿæ ·æœ¬ï¼ˆæ¶‰åŠåŠ¨è¯æ›¿æ¢å’Œå®ä½“æ›¿æ¢ï¼Œå¹¶ä¸”äººå·¥æ ‡æ³¨çš„è´Ÿæ ·æœ¬ä¹ŸåŒ…æ‹¬åœ¨å†…ï¼‰
> To generate up to k = 8 negative candidates x<sup>âˆ’</sup>, we follow a perturb-and-generate strategy for each utterance in the training data.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 7](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=7&annotation=W8MHH82Z))  
> ç¬”è®°ï¼šä½¿ç”¨æ‰°åŠ¨ç”Ÿæˆçš„ç­–ç•¥è¿›è¡Œè´Ÿæ ·æœ¬ç”Ÿæˆ
```


## Baselines
[]()       [*Code*]()
> 

## å®éªŒè¯„æµ‹
![[Pasted image 20220706165712.png]]

![[Pasted image 20220706181804.png]]
```ad-note
title: çœ‹æ³•
CTRLçš„æ§åˆ¶åœ¨å¤§å¤šæ•°è¯„ä»·æŒ‡æ ‡ä¸Šéƒ½å–å¾—äº†æœ€ä¼˜æ•ˆæœï¼Œä½†æ˜¯å¯¹æµç•…æ€§ç›¸å…³çš„æŒ‡æ ‡å¹¶ä¸å‹å¥½ï¼Œä¾‹å¦‚BLEUå’ŒROUGEï¼Œè¿™ä¸ªä»ä¾§é¢è¯´æ˜æ¨¡å‹æ²¡æœ‰å­¦ä¼šåˆç†åˆ©ç”¨çŸ¥è¯†ï¼Œæˆ–è®¸å¯ä»¥ä½œä¸ºä»¥åä¼˜åŒ–çš„ä¸€ä¸ªç‚¹

```
![[Pasted image 20220706182727.png]]
```ad-note
title: çœ‹æ³•
å¯¹æ¯”å­¦ä¹ å¥½å¼ºå¤§ğŸ‘ğŸ»

```
![[Pasted image 20220706183541.png]]

![[Pasted image 20220706183656.png]]

## å®éªŒç»“è®º


## ç¬”è®°
> two versions of the same dialogue turn, either hallucinated or faithful, can provide signal for (contrastive) learning and evidence for a linguistic analysis.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 3](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=3&annotation=DZG3BP7V))  
> ç¬”è®°ï¼šåŒæ ·åœ°ä¸Šä¸‹æ–‡ï¼Œä¸åŒç±»å‹çš„å›å¤ğŸ‘ğŸ»

> WOW is relatively less hallucinated compared to CMU-DoG and TopicalChat. Moreover, full hallucinationsâ€”responses that contain no faithful content and that therefore need to be entirely thrown outâ€” are highly prevalent in the latter two (61.4% in CMU-DoG and 46.8% in TopicalChat and only 19.7% in WOW). Moreover, knowledge snippets in WOW tend to be shorter, which is preferable as their length is correlated with increased hallucination due to the constrained cognitive capacity for text navigation and comprehension in humansÂ Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 3](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=3&annotation=D4P7R7LA))  
> ç¬”è®°ï¼šä¸ºä»€ä¹ˆé€‰æ‹©WOWè¿›è¡Œæ”¹å†™æ ‡æ³¨çš„åŸå› ï¼šå®Œå…¨å¹»è§‰å›å¤è¾ƒå°‘ã€çŸ¥è¯†çŸ­

> To ensure that the responses are creative, we disallowed workers from copying segments from K.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 4](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=4&annotation=SFEHZKB7))  
> ç¬”è®°ï¼šä¸å…è®¸ç›´æ¥æ‹·è´çŸ¥è¯†

> Although the SEEKER has no restrictions on their utterances, it is inevitable that the conversation may drift awayâ€” because of the edits on the WIZARDâ€™s responseâ€”making the existing SEEKERâ€™s next utterance in WOW incoherent with the new context.Â Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 4](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=4&annotation=3UDV5AMC))  
> ç¬”è®°ï¼šä¸ºäº†è®©åæ–‡ä¸ä¿®æ”¹åçš„ä¸Šæ–‡ä¸€è‡´ï¼Œéœ€è¦ä¿®æ”¹Seekerçš„è¯è¯­
> ![[Pasted image 20220706161150.png]]
> ![[Pasted image 20220706161203.png]]

> Finally, we adopt a training strategy, called loss truncation (Kang and Hashimoto, 2020) to cope with the presence of hallucination in WOW, by adaptively eliminating examples with a high training lossÂ Â Â Â  ([Dziri ç­‰ã€‚, 2022, p. 7](zotero://select/library/items/CRKEZYRQ)) ([pdf](zotero://open-pdf/library/items/RYTSH7UG?page=7&annotation=F84RRJ44))  
> ç¬”è®°ï¼šä½¿ç”¨æŸå¤±æˆªæ–­å»é™¤é«˜æŸå¤±æ ·ä¾‹çš„å½±å“