---
Type: unpublished
Authors: Yi-Lin Tuan, Yun-Nung Chen, Hung-yi Lee
Year: 2019
Title: DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs
Journal: arxiv:1910.00610 [cs]
DOI: 
Publisher: arXiv
---

#  (2019)DyKgChat: Benchmarking Dialogue Generation Grounding on Dynamic Knowledge Graphs
###                  ——Yi-Lin Tuan, Yun-Nung Chen, Hung-yi Lee
[*Read it now! See in Zotero*](zotero://select/items/@DyKgChatBenchmarkingDialogue2019tuan)
**Web:** [Open online](http://arxiv.org/abs/1910.00610)
**Citekey:** DyKgChatBenchmarkingDialogue2019tuan
**Tags:** #数据集 #粗读 #未完
**Code:** [Available Here](https://github.com/Pascalson/DyKGChat)


## 摘要
Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions.

## 总结

  
## 研究动机


## 研究背景/ 问题描述


## 方法的创新点


## 实验评测


## 实验结论


## 笔记
