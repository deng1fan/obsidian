[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2106.09685.pdf)

自然语言处理的一个重要范式包括对一般领域数据的大规模预训练和对特定任务或领域的适应。当预训练更大的模型时，重新训练所有模型参数的完全微调变得不太可行。以GPT-3 175B为例，部署经过精细调整的模型的独立实例（每个实例都有175B参数）成本高昂。论文提出了**低秩（LOW-RANK）自适应（LoRA）**，它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入Transformer架构的每一层，从而大大减少了下游任务的可训练参数数量。与用Adam微调的GPT-3175B相比，LoRA可以将可训练参数的数量减少10000倍，GPU内存需求减少3倍。LoRA在RoBERTa、DeBERTa、GPT-2和GPT-3上的模型质量方面表现相当或优于微调，尽管具有较少的可训练参数、较高的训练吞吐量，并且与适配器不同，没有额外的推理延迟。论文还对语言模型适应中的等级缺陷进行了实证研究，这揭示了LoRA的有效性。

## 1 介绍

自然语言处理中的许多应用依赖于使一个大规模的、预先训练的语言模型适应多个下游应用。这种调整通常通过微调来完成，微调会更新预训练模型的所有参数。微调的主要缺点是新模型包含与原始模型相同多的参数。随着更大的模型每隔几个月进行一次训练，这从GPT-2（Radford等人，b）或RoBERTa大型（Liu等人，2019）的“不方便”转变为GPT-3（Brown等人，2020）的关键部署挑战，具有1750亿个可训练参数。

许多人试图通过仅调整一些参数或学习外部模块以适应新任务来缓解这种情况。这样，除了每个任务的预训练模型外，只需要存储和加载少量特定于任务的参数，大大提高了部署时的操作效率。然而，现有技术通常通过扩展模型深度或减少模型的可用序列长度来引入推理延迟（Houlsby等人，2019年；Rebuffi等人，2017年）（Li&Liang，2021；Lester等人，2021；Hambardzumyan等人，2020年；Liu等人，2021）。更重要的是，这些方法往往无法与微调基线相匹配，从而在效率和模型质量之间产生了权衡。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/a25b7446fb159811aac0d9507aa3530d_MD5.webp)

图1 重新参数化，只训练A和B。

论文从Li等人（2018a）获得灵感；Aghajanyan等人（2020）表明，学习的过度参数化模型实际上存在于低内在维度上。假设在模型自适应过程中权重的变化也具有较低的“内在秩”，所以论文提出了低秩自适应（LoRA）方法。LoRA允许通过优化适应过程中密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练的权重不变，如图1所示。以GPT-3175B为例，论文表明，即使当全秩（即d）高达12,288时，非常低的秩（即图1中的r可以是一个或两个）也足够了，从而使LoRA既具有存储效率又具有计算效率。

**LoRA具有几个关键优势。**

-   预训练的模型可以共享，并用于为不同的任务构建许多小型LoRA模块。通过替换图1中的矩阵A和B，可以冻结共享模型参数并高效地切换任务，从而显著降低存储需求和任务切换开销。
-   当使用自适应优化器时，LoRA使训练更有效，并将硬件进入障碍降低3倍，因为不需要计算梯度或维护大多数参数的优化器状态。相反，只优化注入的小得多的低秩矩阵。
-   简单的线性设计允许在部署时将可训练矩阵与冻结权重合并，与完全微调的模型相比，通过构造，不会引入推理延迟。

虽然论文的提议与训练目标无关，但将语言建模作为论文的激励用例。下面是对语言模型问题的简要描述，特别是在特定任务提示下条件概率的最大化。

假设给出了一个由Φ参数化的预训练自回归语言模型  。

例如，  可以是基于Transformer架构的通用多任务学习器，例如GPT（Radford等人，b；Brown等人，2020）（Vaswani等人，2017）。考虑将这种预训练的模型用于下游条件文本生成任务，例如摘要、机器阅读理解（MRC）和自然语言到SQL（NL2SQL）。每个下游任务由上下文-目标对的训练数据集表示：  ，其中xi和yi都是标记序列。例如，在NL2SQL中，  是一个自然语言查询，  是其对应的SQL命令；对于文本摘要，  是一篇文章的内容，  是它的摘要总结。

在完全微调期间，通过重复遵循梯度将模型初始化为预训练的权重  ，并更新为  ，以最大化条件语言模型目标：

**完全微调**的一个主要缺点是，对于每个下游任务，学习一组不同的参数  ，其维数  等于  。因此，如果预训练的模型很大（例如GPT-3的  ≈1750亿），那么存储和部署许多独立的微调模型实例可能是一项挑战，如果可行的话。

在本文中，采用了一种更有效的参数方法，其中任务特定参数增量  由一组更小的参数   进一步编码。因此，求  的任务变为在Θ上进行优化：

论文着手解决的问题绝非新问题。自迁移学习开始以来，已有数十项工作试图使模型自适应更具参数和计算效率。以语言模型为例，在有效适应方面有两种突出的策略：添加适配器层（Houlsby等人，2019年；Rebuffi等人，2017年；Pfeiffer等人，2021；Rüucklíe等人，2020年）或优化输入层激活的某些形式（Li&Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。然而，这两种策略都有其局限性，尤其是在大规模和延迟敏感的生产场景中。

**适配器层引入推断延迟** 适配器有多种变体。论文专注于Houlsby等人（2019）的原始设计，每个Transformer块有两个适配器层，Lin等人（2020）的最新设计，每个块只有一个，但有一个额外的LayerNorm（Ba等人，2016）。虽然可以通过修剪层或利用多任务设置来减少总延迟（Rüucklíe et al.，2020；Pfeiffer et al.（2021）），但没有直接的方法可以绕过适配器层中的额外计算。这似乎不是问题，因为适配器层被设计为具有很少的参数（有时小于原始模型的1%），因为它具有较小的瓶颈维度，这限制了它们可以添加的FLOP。然而，大型神经网络依靠硬件并行性来保持低延迟，适配器层必须按顺序处理。这在批处理大小通常只有一个的在线推断设置中产生了差异。在没有模型并行性的通用场景中，例如在GPT-2（Radford等人，b）介质上在单个GPU上运行推理，可以看到使用适配器时延迟显著增加，即使瓶颈维度很小（表1）。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/74a351cb01e08507783f76aac4d2af09_MD5.webp)

表1：GPT-2介质中单次正向传递的推断延迟（以毫秒为单位），100次试验的平均值。使用NVIDIA Quadro RTX8000。“|Θ|”表示适配器层中可训练参数的数量。AdapterL和AdapterH是适配器调优的两种变体。在在线、短序列长度场景中，适配器层引入的推断延迟可能很重要。

当需要像Shoeybi等人（2020）所做的那样分割模型时，这个问题会变得更糟；Lepikhin等人（2020），因为额外的深度需要更多同步GPU操作，如AllReduce和Broadcast，除非我们多次冗余存储适配器参数。

**直接优化提示（Prompt）很难** 另一个方向，如前缀调整（Li&Liang，2021），面临着不同的挑战。论文观察到前缀调整很难优化，并且其性能在可训练参数中非单调变化，证实了原始论文中的类似观察结果。更重要的是，保留一部分序列长度用于自适应必然会减少可用于处理下游任务的序列长度，论文怀疑与其他方法相比，这会降低提示的性能。

## 2 方法

### 2.1 低秩参数化更新矩阵

神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有全秩。当适应特定任务时，Aghajanyan等人（2020）表明，预训练的语言模型具有较低的“内在维度”，尽管随机投影到较小的子空间，但仍然可以有效地学习。受此启发，论文假设权重的更新在适应期间也具有较低的“内在秩”。对于预训练的权重矩阵，通过  用低秩分解  表示后者来约束其更新，其中  ，以及秩  。在训练期间，  被冻结并且不接收梯度更新，而A和B包含可训练参数。注意，  和  都与相同的输入相乘，并且它们各自的输出矢量在坐标方向上相加。对于  ，修改的前向传递产生： 在图1中说明了重新参数化。对  使用随机高斯初始化，对  使用零初始化，因此 在训练开始时为零。然后，用  缩放  ，其中α是r中的常数。当使用Adam进行优化时，如果适当缩放初始化，则调整α与调整学习率大致相同。因此，我们只需将α设置为尝试的第一个r，而不进行调整。当改变r时，这种缩放有助于减少重新调整超参数的需要（Yang&Hu，2021）。

**全微调的推广** 更一般的微调形式允许训练预训练参数的子集。LoRA更进一步，并且不需要对权重矩阵进行累积梯度更新以在自适应期间具有完整的秩。这意味着，当将LoRA应用于所有权重矩阵并训练所有权重时，通过将LoRA秩r设置为预训练的权重矩阵的秩来大致恢复完全微调的表现力。换句话说，当增加可训练参数的数量时，**训练LoRA大致收敛于训练原始模型，而基于适配器的方法收敛于MLP**，**而基于前缀的方法则收敛于不需要长输入序列的模型**。

**无其他推断延迟** 当部署到生产环境中时，可以显式地计算和存储  ，并照常执行推理。注意  和BA均为  。当我们需要切换到另一个下游任务时，我以通过减去 然后添加不同的  来恢复  ，这是一个快速操作，内存开销非常小。关键的是，这保证了与通过构造进行微调的模型相比，不会在推理期间引入任何额外的延迟。

### 2.2 将LoRA应用于Transformer

原则上，可以将LoRA应用于神经网络中的**任何权重矩阵子集**，以**减少可训练参数的数量**。在Transformer架构中，自注意力模块中有四个权重矩阵（  ），MLP模块中有两个权重矩阵。将  视为维度  的单个矩阵，即使输出维度通常被分割为注意力头部。为了简单和参数效率，将研究局限于**仅调整下游任务的注意力权重**，并**冻结MLP模块**（因此，它们在下游任务中未进行训练）。

**实际好处和限制** 最显著的好处来自**内存和存储使用量的减少**。对于用Adam训练的大型Transformer，如果  ，将VRAM使用量减少了2/3，因为不需要存储冻结参数的优化器状态。在GPT-3175B上，将训练期间的VRAM消耗从1.2TB减少到350GB。当r＝4并且仅调整查询和值投影矩阵时，检查点大小大约减少了10000×（从350GB减少到35MB）。这使得可以使用更少的GPU进行训练，并避免I/O瓶颈。另一个好处是，**可以在部署时以更低的成本切换任务**，**只需交换LoRA权重**，而不是所有参数。这允许创建许多定制模型，这些模型可以在将预训练的权重存储在VRAM中的机器上进行动态交换。还观察到，与完全微调相比，GPT-3175B训练期间的速度提高了25%，因为不需要计算绝大多数参数的梯度。

## 3 实验

评估了LoRA在**RoBERTa**（Liu等人，2019年）、**DeBERTa**（He等人，2021）和**GPT-2**（Radford等人，b）上的下游任务性能，然后再扩展到GPT-3 175B（Brown等人，2020年）。实验涵盖了广泛的任务，从自然语言理解（NLU）到生成（NLG）。具体而言，评估了RoBERTa和DeBERTa的GLUE（Wang等人，2019）基准。遵循Li&Liang（2021）在GPT-2上的设置进行直接比较，并添加WikiSQL（Zhong等人，2017）（NL到SQL查询）和SAMSum（Gliwa等人，2019）（对话摘要）用于GPT-3上的大规模实验。使用NVIDIA Tesla V100进行所有实验。

### 3.1 基线

为了广泛地与其他基线进行比较，论文复制了先前工作中使用的设置，并在可能的情况下重用其报告的数字。然而，这意味着某些基线可能只出现在某些实验中。

**微调（FT）**是一种常见的自适应方法。在微调过程中，模型被初始化为预先训练的权重和偏差，所有模型参数都进行梯度更新。一个简单的变体是只更新一些层，而冻结其他层。论文在GPT-2之前的工作中报告了一个这样的基线（Li&Liang，2021），它只适用于最后两层（ ）。

**仅偏置或BitFit** 是一个基线，只训练偏置向量，而冻结其他所有向量。与此同时，BitFit也对该基线进行了研究（Zaken等人，2021）。

**前缀嵌入调整（PreEmbed）**在输入标记中插入特殊标记。这些特殊标记具有可训练的单词嵌入，通常不在模型的词汇表中。放置此类token的位置可能会影响性能。我们关注的是“前缀”，它在提示前加上这样的标记，以及“中缀”，它附加在提示后面；Li和Liang（2021）对这两者进行了讨论。我们使用lp（[resp.li](https://link.zhihu.com/?target=http%3A//resp.li/)）表示前缀（resp.infix）标记的数量。可训练参数的数量为  。

**前缀层调整（PreLayer）**是前缀嵌入调整的扩展。学习每个Transformer层之后的激活，而不是仅仅学习一些特殊token的单词嵌入（或者等效地，嵌入层之后的启动）。从先前层计算的激活被可训练的激活简单地替换。得出的可训练参数数为  ，其中L是Transformer层数。

**适配器调整** Houlsby等人（2019）提出的适配器调整在自注意力模块（和MLP模块）和后续残差连接之间插入适配器层。适配器层中有两个完全连接的层，它们之间具有非线性。称这种原始设计为  。最近，Lin等人（2020）提出了一种更有效的设计，其中适配器层仅在MLP模块之后和LayerNorm之后应用。称之为。这与  Pfeiffer等人（2021）提出的另一种设计非常相似，称之为  。还包括另一个基准调用AdapterDrop（Råucklåe et al.，2020），它删除了一些适配器层以提高效率（  ）。尽可能引用先前作品中的数字，以最大化论文所比较的基线数量。在所有情况下，都有  ，其中，  是适配器层的数量，而ΒLLN是可训练层规范的数量（例如，在AdapterL中）。

**LoRA** 将可训练的秩分解矩阵对与现有权重矩阵并行添加。如第4.2节所述，为了简单起见，在大多数实验中仅将LoRA应用于  和  。可训练参数的**数量由秩r和原始权重**的形状决定：  。其中，  是应用LoRA的权重矩阵的数量。

### 3.2 **ROBERT**_A_ BASE/LARGE

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/627a1bd300aac5d6437615effc3d502c_MD5.webp)

表2:RoBERTabase、RoBERTalarge和DeBERTaXXL在GLUE基准上采用不同的适应方法。报告了MNLI的总体（匹配和不匹配）准确性、CoLA的Matthew相关性、STS-B的Pearson相关性以及其他任务的准确性。所有指标越高越好。*表示先前作品中发表的数字。†表示在类似于Houlsby等人（2019）的设置中配置的运行

RoBERTa（Liu等人，2019）优化了最初在BERT中提出的预训练配方（Devlin等人，2019a），并在不引入更多可训练参数的情况下提高了后者的任务性能。尽管近年来，RoBERTa在NLP排行榜上被GLUE基准等更大的模型所取代（Wang等人，2019），但由于其规模在从业者中仍然是一个具有竞争力的、受欢迎的预训练模型。从HuggingFace Transformers库（Wolf等人，2020）中获取了经过预训练的RoBERTa base（125M）和RoBERTa large（355M），并根据GLUE基准评估了不同高效适应方法在任务上的性能。为了确保公平的比较，在与适配器进行比较时对如何评估LoRA进行了两个关键的更改。首先，对所有任务**使用相同的批次大小**，并使用**128的序列长度**来匹配适配器基线。其次，将模型初始化为MRPC、RTE和STS-B的预训练模型，而不是像微调基线那样已经适应MNLI的模型。按照Houlsby等人（2019）的这种更受限制的设置运行，标记为†。结果见表2（前三节）。有关所用超参数的详细信息，请参见第D.1节。

### 3.3 DEBERTA XXL

DeBERTa（He等人，2021）是BERT的一个较新变体，它在更大范围内接受训练，在GLUE（Wang等人，2019）和SuperGLUE等基准上表现非常有竞争力（Wang等，2020）。论文评估LoRA是否仍能与GLUE上完全微调的DeBERTa XXL（1.5B）的性能相匹配。结果见表2（底部）。

### 3.4 GPT-2 MEDIUM/LARGE

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/1f87ce1fc6aa665dee9e69cdc85761bf_MD5.webp)

表3:GPT-2中等（M）和大（L），具有E2E NLG挑战的不同适应方法。对于所有指标，越高越好。LoRA以可比或更少的可训练参数优于多个基线。运行的实验显示了置信区间。*表示先前作品中发表的数字。

已经表明LoRA可以是NLU上完全微调的一个竞争性替代方案，论文希望回答LoRA是否仍然在NLG模型上占优势，例如GPT-2中型和大型（Radford等人，b）。为了进行直接比较，将设置尽可能靠近Li&Liang（2021）。由于空间限制，仅在本节中介绍E2E NLG挑战的结果（表3）。

### 3.5 扩展到GPT-3 175B

作为LoRA的最终压力测试，使用1750亿个参数扩展到GPT-3。由于训练成本高，只报告随机种子上给定任务的典型标准差，而不是为每个条目提供一个标准差。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/f0a7fab7ca40d7560d95f52049a087cc_MD5.webp)

表4:GPT-3175B上不同适应方法的性能。报告了WikiSQL上的逻辑表单验证准确性、MultiNLI匹配的验证准确性以及SAMSum上的Rouge-1/2/L。LoRA比以前的方法表现更好，包括完全微调。WikiSQL上的结果在三个指标上的波动约为±0.5%，MNLI-m约为±0.1%，SAMSum约为±0.2/±0.2/？.1。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/bc220ba05f268e9433b34c0b61e1ca8d_MD5.webp)

图2:GPT-3 175B验证精度与WikiSQL和MNLI上匹配的几种适应方法的可训练参数的数量。LoRA具有更好的可扩展性和任务性能。

如表4所示，LoRA在所有三个数据集上都匹配或超过微调基线。注意，并非所有方法都从具有更多可训练参数中单调受益，如图2所示。当使用256个以上的特殊tokens进行前缀嵌入调优或使用32个以上的专用tokens进行前缀层调优时，观察到性能显著下降。这证实了Li和Liang（2021）的类似观察结果。虽然对这一现象的彻底调查超出了这项工作的范围，但怀疑拥有更多特殊tokens会导致输入分布进一步偏离训练前的数据分布。

## 4 相关工作

### 4.1 Transformer语言模型 

Transformer（Vaswani等人，2017）是一种序列到序列架构，大量使用自注意力。Radford等人（a）通过使用Transformer解码器堆栈将其应用于自回归语言模型。从那时起，基于Transformer的语言模型主导了NLP，在许多任务中达到了最先进的水平。BERT（Devlin et al.，2019b）和GPT-2（Radford et al.，b）出现了一种新的范式——两者都是在大量文本上训练的大型Transformer语言模型——其中在对通用域数据进行预训练后对任务特定数据进行微调，与直接对任务特定的数据进行训练相比，可以显著提高性能。训练更大的Transformer通常会产生更好的性能，这仍然是一个积极的研究方向。GPT-3（Brown等人，2020）是迄今为止使用175B参数训练的最大的单Transformer语言模型。

### 4.2 Prompt工程和微调 

虽然GPT-3175B可以通过几个额外的训练示例来调整其行为，但结果在很大程度上取决于输入提示（Brown等人，2020）。这就需要一种经验艺术来编写和格式化提示，以最大化模型在所需任务上的性能，这就是所谓的**提示工程或提示黑客**。微调将在一般域上预训练的模型重新训练为特定任务Devlin等人（2019b）；Radford等人（a）。其变体包括仅学习参数Devlin等人的子集（2019b）；Collbert&Weston（2008），但经常对他们进行再训练，以最大限度地提高下游性能。然而，GPT-3175B的巨大性使得以通常的方式执行微调具有挑战性，这是因为它产生的检查点很大，并且由于它具有与预训练相同的内存占用，因此硬件进入壁垒很高。

4.3 参数有效适应

许多人已经提出在神经网络中的现有层之间插入适配器层（Houlsby等人，2019；Rebuffi等人，2017；Lin等人，2020）。论文的方法使用类似的瓶颈结构来对权重更新施加低秩约束。关键的功能差异在于，论文学习的权重可以在推理过程中与主权重合并，因此不会引入任何延迟，适配器层的情况并非如此（第3节）。适配器的一个共同扩展是COMPACTER（Mahabadi et al.，2021），它基本上使用带有一些预定权重分配方案的Kronecker产品对适配器层进行参数化。类似地，将LoRA与其他基于张量积的方法相结合可以潜在地提高其参数效率，这将留给未来的工作。最近，许多人提出优化输入词嵌入以代替微调，类似于prompt工程的连续可微泛化（Li&Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。在路问的实验部分，将与Li&Liang（2021）进行比较。然而，这一行只能通过在提示中使用更多特殊的标记来扩展，当学习位置嵌入时，这些标记占用了任务标记的可用序列长度。

### 4.3 深度学习中的低秩结构

低秩结构在机器学习中非常常见。许多机器学习问题具有一定的内在低秩结构（Li等人，2016；Cai等人，2010；Li等人，2018b；Grasedyck等人，2013）。此外，众所周知，对于许多深度学习任务，尤其是那些具有严重过参数化神经网络的任务，经过训练后，学习的神经网络将具有低秩属性（Oymak等人，2019）。以前的一些工作甚至在训练原始神经网络时明确施加了低秩约束（Sainath等人，2013；Povey等人，2018；Zhang等人，2014；Jaderberg等人，14；Zhao等人，2016；Khodak等人，2021；Denil等人，2014）；然而，据我们所知，这些工作中没有一项考虑低秩更新到冻结模型以适应下游任务。在理论文献中，已知当基础概念类具有一定的低秩结构时，神经网络优于其他经典学习方法，包括相应的（有限宽度）神经正切核（Allen Zhu等人，2019；Li&Liang，2018）（Ghorbani等人，2020；Allen Zhu&Li，2019；Allen朱&Li，2020a）。Allen Zhu&Li（2020b）的另一个理论结果表明，低秩的适应对**对抗性训练**很有用。

## 5 理解低秩更新

鉴于LoRA的经验优势，希望进一步解释从下游任务中学习到的低秩适应属性。注意，低秩结构不仅降低了硬件进入壁垒，这允许并行运行多个实验，而且还提供了更新权重与预训练权重如何相关的更好的解释性。论文将研究重点放在GPT-3175B上，实验中实现了可训练参数的最大减少（最多10000×），而不会对任务性能产生不利影响。

论文进行了一系列实证研究，以回答以下问题：

1）给定参数预算约束，应该调整预训练Transformer中的权重矩阵子集，以最大化下游性能？

2） “最佳”适应矩阵∆W真的有效吗？如果是，那么在实践中使用什么是好的rank？

3） ∆W和W之间的关系是什么？∆W是否与W高度相关？∆W与W相比有多大？

### 5.1 应该将LoRA应用于Transformer中的哪些权重矩阵？

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/3ee30d6d2b2e38b1792ff81252cc3f6f_MD5.webp)

表5：在给定相同数量的可训练参数的情况下，将LoRA应用于GPT-3中不同类型的注意力权重后，WikiSQL和MultiNLI的验证准确性。同时调整Wq和Wv可获得最佳的整体性能。发现，对于给定的数据集，随机种子的标准偏差是一致的，在第一列中报告了这一点。

给定有限的参数预算，应该使用LoRA调整哪些类型的权重，以获得下游任务的最佳性能？如第4.2节所述，只考虑自注意力模块中的权重矩阵。论文在GPT-3175B上设置了18M的参数预算（如果存储在FP16中，大约为35MB），如果调整一种类型的注意力权重，则对应于r＝8，如果对所有96层调整两种类型，则对应r＝4。结果见表5。

注意，将所有参数设置为∆Wq或∆Wk会显著降低性能，而调整Wq和Wv会产生最佳结果。这表明，即使是rank=4也能捕获∆W中的足够信息，因此，与采用更大rank的单一类型权重相比，更适合采用更多权重矩阵。

### 5.2 LoRA的最佳rank是多少？

将注意力转向秩r对模型性能的影响。调整{Wq，Wv}、{Wq、Wk、Wv、Wc}，并仅调整Wq进行比较。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/4edc7d4f5f361104efbafff4b09f2629_MD5.webp)

表6：不同r的WikiSQL和MultiNLI的验证精度。令我们惊讶的是，小的r足以适应这些数据集上的Wq和Wv，而单独训练Wq需要更大的r。

表6显示，令人惊讶的是，LoRA已经以非常小的r表现出了竞争力（对于{Wq，Wv}比仅Wq更是如此）。这表明更新矩阵∆W可能具有非常小的“内在秩”。为了进一步支持这一发现，检查了通过不同的r选择和不同的随机种子学习的子空间的重叠。论文认为增加r并不能覆盖更有意义的子空间，这表明**低秩自适应矩阵就足够了**。

**不同r的子空间相似性** 给定  和  ，它们是使用相同的预训练模型的秩r＝8和64的学习自适应矩阵，进行奇异值分解，得到了正确的奇异酉矩阵  和  。我们希望回答：  中的前i个奇异向量所覆盖的子空间中有多少包含在  的前j个奇异向量覆盖的子中？论文使用基于格拉斯曼距离的归一化子空间相似性来测量这个量。

其中  表示对应于前i个奇异向量的  的列。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/5d6e0c7ccacc2e19e78943cd711bda48_MD5.webp)

图3：∆Wq和∆Wv的Ar=8和Ar=64列向量之间的子空间相似性。第三个和第四个图形放大了前两个图形中的左下三角形。r=8中的顶部方向包含在r=64中，反之亦然。

φ（·）的范围为[0，1]，其中1表示子空间的完全重叠，0表示完全分离。如图3所示，φ随i和j的变化而变化。由于空间限制，我们只观察了第48层（96层中），但结论也适用于其他层。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/fa65f5cf3ad8f7f2edd2b75311773fba_MD5.webp)

图4：左侧和中间：第48层中∆Wq和∆Wv的两个随机种子的Ar=64列向量之间的归一化子空间相似性。右图：两个随机高斯矩阵的列向量之间的相同热图。

从图3中得出了一个重要的观察结果。 对应于顶部奇异向量的方向在  和  之间显著重叠，而其他方向则不重叠。具体而言，  的  （  ）和  的  （ ）共享维度1的子空间，归一化相似度>0.5，这解释了为什么r=1在GPT-3的下游任务中表现得很好。

由于  和  都是使用相同的预训练模型学习的，图3表明  与  的顶部奇异向量方向是最有用的，而其他方向可能包含训练期间积累的大部分随机噪声。因此，自适应矩阵确实可以具有非常低的秩。

### 5.3 自适应矩阵∆W与W相比如何？

论文进一步研究了∆W和W之间的关系。特别是，∆W是否与W高度相关？（或者从数学上讲，∆W主要包含在W的顶部奇异方向中吗？）此外，∆W与W中的相应方向相比有多大？这可以揭示适应预训练语言模型的潜在机制。

为了回答这些问题，通过计算  将W投影到∆W的r维子空间上，其中U/V是∆W左/右奇异向量矩阵。然后，比较了  和  之间的F范数。作为比较，还通过将U，V替换为W的前r个奇异向量或随机矩阵来计算  。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/9fa3127e48ed1c81a4a003ba1b70553c_MD5.webp)

表7：U^TWqV^T的Frobenius范数，其中U和V是（1）∆Wq、（2）Wq或（3）随机矩阵的左/右上r个奇异向量方向。权重矩阵取自GPT-3的第48层。

从表7中得出几个结论。首先，与随机矩阵相比，∆W与W的相关性更强，表明∆W放大了W中已经存在的一些特征。其次，∆W只放大W中未强调的方向，而不是重复W的顶部奇异方向。第三，放大系数相当大：当r=4时，放大系数为21.5≈6.91/0.32。这表明，**低秩适应矩阵潜在地放大了在一般预训练模型中学习但未强调的特定下游任务的重要特征**。

![](../../../_resources/LoRA%20%E2%80%94%E2%80%94%20%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8E%E7%A7%A9%E9%80%82%E5%BA%94/9b54c10bd39a16fca50d52d6a131cc99_MD5.webp)

96层Transformer中第1层、第32层、第64层和第96层的∆Wq和∆Wv的列向量Ar=8和Ar=64之间的归一化子空间相似性。